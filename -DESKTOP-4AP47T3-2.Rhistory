X_list[[j]]=as.matrix(DatosPorBanco[[j]][,c(2,3,5,6)])
}
Y_list =list()
for (j in 1:length(NamesBancos)) {
Y_list[[j]]=as.matrix(DatosPorBanco[[j]][,8])
}
set.seed(NULL)
#Tamano de la base de datos
n <- nrow(X)
#Cantidad de atributos o caracterÃ­sticas de la base
p <- ncol(X)
n_clients=length(X_list)
mu = 20
epsilon=5
C=1
m=n/n_clients
DeltaSU=2*C/(m*epsilon)
SigmaU=DeltaSU
DP=0
T=100
#Guardalas actualizaciones globales de Omega (como vectores columna)
Acz<- matrix(0, nrow = p, ncol = T)
#Guaradara historial de errores
Error<- rep(0,T)
# Inicializa omega en 0
omega <- Acz[,1]
ParLocales=matrix(0, nrow = p, ncol = length(X_list))
for (l in 1:T) {
omega_global <- Acz[,l]
for (m in 1:length(X_list)) {
LocalData=as.matrix(X_list[[m]])
rownames(LocalData) <- NULL
colnames(LocalData) <- NULL
ResultadoLocal <- ppa(LocalData, Y_list[[m]], omega_global)
if (DP==1){
ResultadoLocal=ResultadoLocal+
rnorm(length(ResultadoLocal), mean = 0, sd = SigmaU)
}
ParLocales[,m]=ResultadoLocal
}
#Promedio de parametros
Promedio=apply(ParLocales[ ,1:5], 1, mean, na.rm = TRUE)
#Actualizamos omega global
if (l==T){
print(Promedio)
} else{
Acz[,l+1]<- Promedio
}
Data=as.matrix(X)
rownames(Data) <- NULL
colnames(Data) <- NULL
Error[l]=mse(Promedio,X,as.vector(Y))
}
Error
plot(Error, type="l", xlab  =paste0("Iteración"),
main = expression("Error "),
#ylim = c(0,130)
)
if (DP==1){
plot(Error, type="l", xlab  =paste0("Número de Iteración"), ylab="Error" ,
main = paste0(expression("Evolución del Error DP")))
legend("topright", # PosiciÃ³n de la leyenda
legend=c(paste("Epsilon =", epsilon),
#paste("# de iteraciones =", IT),
paste("Umbral de recorte=", C),
paste("Coeficiente de penalización=", mu)
), # Texto de la leyenda
#col="black", # Color de la lÃ­nea
#lty=1, # Tipo de lÃ­nea
bty="n") # Eliminar el marco de la leyenda
} else{
plot(Error, type="l", xlab  =paste0("Número de Iteración"), ylab="Error" ,
main = paste0(expression("Evolución del Error")))
legend("topright", # PosiciÃ³n de la leyenda
legend=c(#paste("Epsilon =", epsilon),
#   paste("# de iteraciones =", IT),
paste("Umbral de recorte=", C),
paste("Coeficiente de penalización=", mu)
),
#paste("sigma=", sigma)
#), # Texto de la leyenda
#col="black", # Color de la lÃ­nea
#lty=1, # Tipo de lÃ­nea
bty="n") # Eliminar el marco de la leyenda
}
#Graficando diferentes valores de epsilon
Valepsilon=c(0.4,0.9,1.5,2,10,100)
CostosEpsilon=list()
for (LL in 1:length(Valepsilon) ) {
#Tamano de la base de datos
n <- nrow(X)
#Cantidad de atributos o caracterÃ­sticas de la base
p <- ncol(X)
n_clients=length(X_list)
epsilon=Valepsilon[LL]
mu = 20
C=1
m=n/n_clients
DeltaSU=2*C/(m*epsilon)
SigmaU=DeltaSU
DP=1
#Algorithm 1: Noising before Aggregation FL
#InicializaciÃ³n:
#NÃºmero de iteraciones
T=100
#Guardalas actualizaciones globales de Omega (como vectores columna)
Acz<- matrix(0, nrow = p, ncol = T)
#Guaradara historial de errores
Error<- rep(0,T)
# Inicializa omega en 0
omega <- Acz[,1]
ParLocales=matrix(0, nrow = p, ncol = length(X_list))
for (l in 1:T) {
omega_global <- Acz[,l]
for (m in 1:length(X_list)) {
LocalData=as.matrix(X_list[[m]])
rownames(LocalData) <- NULL
colnames(LocalData) <- NULL
ResultadoLocal <- ppa(LocalData, Y_list[[m]], omega_global)
if (DP==1){
ResultadoLocal=ResultadoLocal+
rnorm(length(ResultadoLocal), mean = 0, sd = SigmaU)
}
ParLocales[,m]=ResultadoLocal
}
#Promedio de parametros
Promedio=apply(ParLocales[ ,1:5], 1, mean, na.rm = TRUE)
#Actualizamos omega global
if (l==T){
print(Promedio)
} else{
Acz[,l+1]<- Promedio
}
Data=as.matrix(X)
rownames(Data) <- NULL
colnames(Data) <- NULL
Error[l]=mse(Promedio,X,as.vector(Y))
}
CostosEpsilon[[LL]]=Error
}
# Configura el grÃ¡fico
plot(CostosEpsilon[[1]], type = "l",lwd=2, col = 1, ylim = range(unlist(CostosEpsilon)),
xlab = "Iteración", ylab = "Costos", main = "Costos variando Epsilón")
# AÃ±ade los demÃ¡s vectores
for (i in 2:length(CostosEpsilon)) {
lines(CostosEpsilon[[i]], type = "l", col = i,lwd=2)
}
# Agrega la leyenda
legend("topright", title = "Valor de epsilon", legend = Valepsilon, col = 1:length(CostosEpsilon),
pch = NA, lty = 1, lwd=2,cex = 0.7)
#Validacion cruzada
mu_val=c(0.001, 0.01, 0.1, 1,2,5,10,20,100,200,500, 1000,2000)
#desde aca para valid Cruzada
NITCrossVal=5
ErorCrossValidation=matrix(rep(0,length(mu_val)*NITCrossVal), ncol = NITCrossVal, nrow = length(mu_val))
for (cross in 1:NITCrossVal) {
contadormiu=1
for (miu in mu_val) {
mu = miu
#Guardalas actualizaciones globales de Omega (como vectores columna)
Acz<- matrix(0, nrow = p, ncol = T)
#Guaradara historial de errores
Error<- rep(0,T)
# Inicializa omega en 0
omega <- Acz[,1]
ParLocales=matrix(0, nrow = p, ncol = length(X_list))
for (l in 1:T) {
omega_global <- Acz[,l]
for (m in 1:length(X_list)) {
LocalData=as.matrix(X_list[[m]])
rownames(LocalData) <- NULL
colnames(LocalData) <- NULL
ResultadoLocal <- ppa(LocalData, Y_list[[m]], omega_global)
if (DP==1){
ResultadoLocal=ResultadoLocal+
rnorm(length(ResultadoLocal), mean = 0, sd = SigmaU)
}
ParLocales[,m]=ResultadoLocal
}
#Promedio de parametros
Promedio=apply(ParLocales[ ,1:5], 1, mean, na.rm = TRUE)
#Actualizamos omega global
if (l==T){
print(Promedio)
} else{
Acz[,l+1]<- Promedio
}
Data=as.matrix(X)
rownames(Data) <- NULL
colnames(Data) <- NULL
Error[l]=mse(Promedio,X,as.vector(Y))
}
#Error
#plot(Error, type="l", xlab  =paste0("IteraciÃ³n"),
#    main = paste0("Error ", "( con mu=", mu, ")" ))
#print(paste0("Error final:", Error[length(Error)], "con mu=",mu ))
#Paso Cross Validation
ErorCrossValidation[contadormiu,cross]=Error[length(Error)]
contadormiu=contadormiu+1
}
}
rowMeans(ErorCrossValidation)
Validiacion_cruzada=data.frame(Coeficiente_de_Penalización=mu_val, Error_Promedio_Obtenido=rowMeans(ErorCrossValidation)
)
print(Validiacion_cruzada)
# Crear una imagen PNG donde dibujaremos la tabla
png("tabla_completa.png", width = 800, height = 600)
# Iniciar el gráfico para poder usar 'text'
plot.new()
# Definir las coordenadas y márgenes para dibujar el texto
par(mar = c(5, 5, 2, 2))  # Ajustamos márgenes
# Dibujar los encabezados de la tabla
text(0.1, 0.9, "Coeficiente_de_Penalización", cex = 1.0)
text(0.6, 0.9, "Error_Promedio_Obtenido", cex = 1.0)
# Dibujar los datos del dataframe en las filas (ajustamos la posición)
for (i in 1:nrow(Validiacion_cruzada)) {
# Dibujar las celdas de la primera columna (Nombre)
text(0.1, 0.8 - (i * 0.06), Validiacion_cruzada$Coeficiente_de_Penalización[i], cex = 1)
# Dibujar las celdas de la segunda columna (Edad)
text(0.6, 0.8 - (i * 0.06), Validiacion_cruzada$Error_Promedio_Obtenido[i], cex = 1)
}
# Finalizar el dispositivo gráfico y guardar la imagen
dev.off()
print(Validiacion_cruzada)
library(VGAM)
library(PerformanceAnalytics)
library(e1071)
library(caret)
library(mlbench)
# Cargar el conjunto de datos iris
data(iris)
iris=data.frame(conjunto_de_datos_esep_hospla_2022$SHP40_H,conjunto_de_datos_esep_hospla_2022$SHP41_H)
test_data <- iris[-sample_index, ]
# Calcular estadísticas (media y desviación estándar) por clase
stats <- list()
# Dividir los datos en conjuntos de entrenamiento y prueba
sample_index <- sample(1:nrow(iris), 0.6 * nrow(iris))
train_data <- iris[sample_index, ]
plot(train_data, col=train_data[,label_col]+1, cex=1.5, pch=19)
#En que columna se encuentran la categoria
label_col <- 3
# Dividir los datos en conjuntos de entrenamiento y prueba
sample_index <- sample(1:nrow(iris), 0.6 * nrow(iris))
train_data <- iris[sample_index, ]
plot(train_data, col=train_data[,label_col]+1, cex=1.5, pch=19)
test_data <- iris[-sample_index, ]
# Calcular estadísticas (media y desviación estándar) por clase
stats <- list()
#En el siguiente ciclo, construrimos stats y guarda la media, sd en cada clase
for (label in unique(train_data[[label_col]])) {
subset <- train_data[train_data[[label_col]] == label,]
stats[[as.character(label)]] <- lapply(subset[, -label_col], function(x) c(mean = mean(x), sd = sd(x)))
}
#En el siguiente ciclo, construrimos stats y guarda la media, sd en cada clase
for (label in unique(train_data[[label_col]])) {
subset <- train_data[train_data[[label_col]] == label,]
stats[[as.character(label)]] <- lapply(subset[, -label_col], function(x) c(mean = mean(x), sd = sd(x)))
}
# Cargar el conjunto de datos iris
data(iris)
iris=data.frame(conjunto_de_datos_esep_hospla_2022$SHP40_H,conjunto_de_datos_esep_hospla_2022$SHP41_H)
# Cargar el conjunto de datos iris
data(iris)
#iris=data.frame(conjunto_de_datos_esep_hospla_2022$SHP40_H,conjunto_de_datos_esep_hospla_2022$SHP41_H)
DATA <- read.csv("diabetes.csv")
PPlot=DATA[,c(2,6,9)]
iris=PPlot
#En que columna se encuentran la categoria
label_col <- 3
# Dividir los datos en conjuntos de entrenamiento y prueba
sample_index <- sample(1:nrow(iris), 0.6 * nrow(iris))
train_data <- iris[sample_index, ]
plot(train_data, col=train_data[,label_col]+1, cex=1.5, pch=19)
test_data <- iris[-sample_index, ]
# Calcular estadísticas (media y desviación estándar) por clase
stats <- list()
#En el siguiente ciclo, construrimos stats y guarda la media, sd en cada clase
for (label in unique(train_data[[label_col]])) {
subset <- train_data[train_data[[label_col]] == label,]
stats[[as.character(label)]] <- lapply(subset[, -label_col], function(x) c(mean = mean(x), sd = sd(x)))
}
#test_data[,c(2,3)]
#train_data[train_data[[label_col]] == 0,]
#train_data[train_data[[label_col]] == 0,2]
#train_data[train_data[[label_col]] == 0,c(2,3)]
#hist(train_data[train_data[[label_col]] == 0,2])
ks.test(train_data[train_data[[label_col]] == 0,1], "pnorm", mean = mean(train_data[train_data[[label_col]] == 0,1]), sd = sd(train_data[train_data[[label_col]] == 0,1]))
ks.test(train_data[train_data[[label_col]] == 1,1], "pnorm", mean = mean(train_data[train_data[[label_col]] == 1,1]), sd = sd(train_data[train_data[[label_col]] == 1,1]))
ks.test(train_data[train_data[[label_col]] == 0,2], "pnorm", mean = mean(train_data[train_data[[label_col]] == 0,2]), sd = sd(train_data[train_data[[label_col]] == 0,2]))
ks.test(train_data[train_data[[label_col]] == 1,2], "pnorm", mean = mean(train_data[train_data[[label_col]] == 1,2]), sd = sd(train_data[train_data[[label_col]] == 1,2]))
# Calcular probabilidades a priori ( p(c_i)  )
total <- nrow(train_data)
priors <- table(train_data[[label_col]]) / total
priors <- as.list(priors)
# Calcular la probabilidad de la distribución normal
gaussian_prob <- function(x, mean, sd) {
if (sd == 0) {
return(ifelse(x == mean, 1, 0))
}
return((1 / (sqrt(2 * pi) * sd)) * exp(-((x - mean)^2 / (2 * sd^2))))
}
# Predecir las etiquetas del conjunto de prueba
ProbaPost<- list()
predictions=list()
for (i in 1:nrow(test_data)){
x <- test_data[i, -label_col]
# prob a posteiori (solo numerador)
#esto es (P(c_j|X_1,\dots X_n)=P(c_j)\prod P(X_i|c_j))
posteriors <- list()
for (label in names(stats)) {
likelihood <- 1
for (feature in names(stats[[label]])) {
prob <- gaussian_prob(x[[feature]], stats[[label]][[feature]]["mean"], stats[[label]][[feature]]["sd"])
likelihood <- likelihood * prob
}
posteriors[[label]] <- likelihood * priors[[label]]
}
ProbaPost[[i]]=posteriors
predictions[i]=names(which.max(unlist(posteriors)))
}
# Verificar las predicciones y la precisión
print(ProbaPost)
print(predictions)
print(test_data[,label_col])
CategPred=substr(as.character(predictions), 1, nchar(as.character(predictions)) - 3)
accuracy <- sum(substr(as.character(predictions), 1, nchar(as.character(predictions)) - 3) == as.character(test_data[,label_col])) / nrow(test_data)
print(paste("Accuracy:", round(accuracy, 2)))
plot(test_data[,c(1,2)], col=test_data[,label_col]+2, cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=as.numeric(CategPred)+2, cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data, col=(test_data[,label_col]==CategPred)+2, cex=1.5, pch=19)
plot(test_data[,c(1,2)], col=test_data[,label_col]+2, cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=as.numeric(CategPred)+2, cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
legend("topleft", legend = c("No Positivo", "Positivo"),ylab="IMC",
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=test_data[,label_col]+2, cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),ylab="IMC",
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=test_data[,label_col]+2,ylab="IMC", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=as.numeric(CategPred)+2,ylab="IMC", cex=1.5, pch=19)
plot(test_data[,c(1,2)], col=test_data[,label_col]+2,ylab="IMC", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=as.numeric(CategPred)+2,ylab="IMC", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=test_data[,label_col]+2,ylab="IMC", xlab="Glucosa", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=as.numeric(CategPred)+2,ylab="IMC",xlab="Glucosa", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
epsilon=0.1
n=10000
#Calculo de Max y Min
Upper=rep(0,ncol(iris)-1)
Lower=rep(0,ncol(iris)-1)
for (j in 1:(ncol(iris)-1)) {
Upper[j]=max(iris[j])
Lower[j]=min(iris[j])
}
Smean=rep(0,ncol(iris)-1)
Ssd=rep(0,ncol(iris)-1)
#Sensibilidades
for (j in 1:(ncol(iris)-1)) {
Smean[j]=(Upper[j]-Lower[j])/(n+1)
Ssd[j]=sqrt(n)*(Upper[j]-Lower[j])/(n+1)
}
#En el siguiente ciclo, construrimos stats y guarda la media, sd en cada clase
for (label in unique(train_data[[label_col]])) {
subset <- train_data[train_data[[label_col]] == label,]
stats[[as.character(label)]] <- lapply(subset[, -label_col], function(x) c(mean = mean(x), sd = sd(x)))
}
#Statsdp
statsdp <- stats
for (i in 1:length(stats)) {
for (j in 1:(ncol(iris)-1)) {
statsdp[[i]][[j]][["mean"]]=stats[[i]][[j]][["mean"]]+rlaplace(1,0,Smean[j]/epsilon)
statsdp[[i]][[j]][["sd"]]=stats[[i]][[j]][["sd"]]+rlaplace(1,0,Ssd[j]/epsilon)
}
}
# Calcular probabilidades a priori ( p(c_i)  )
total <- nrow(train_data)
priors <- table(train_data[[label_col]]) / total
priors <- as.list(priors)
# Calcular la probabilidad de la distribución normal
gaussian_prob <- function(x, mean, sd) {
if (sd == 0) {
return(ifelse(x == mean, 1, 0))
}
return((1 / (sqrt(2 * pi) * sd)) * exp(-((x - mean)^2 / (2 * sd^2))))
}
# Predecir las etiquetas del conjunto de prueba
ProbaPost<- list()
predictions=list()
for (i in 1:nrow(test_data)){
x <- test_data[i, -label_col]
# prob a posteiori (solo numerador)
#esto es (P(c_j|X_1,\dots X_n)=P(c_j)\prod P(X_i|c_j))
posteriors <- list()
for (label in names(statsdp)) {
likelihood <- 1
for (feature in names(statsdp[[label]])) {
prob <- gaussian_prob(x[[feature]], statsdp[[label]][[feature]]["mean"], statsdp[[label]][[feature]]["sd"])
likelihood <- likelihood * prob
}
posteriors[[label]] <- likelihood * priors[[label]]
}
ProbaPost[[i]]=posteriors
predictions[i]=names(which.max(unlist(posteriors)))
}
# Verificar las predicciones y la precisión
print(ProbaPost)
print(predictions)
print(test_data[,label_col])
dpCategPred=substr(as.character(predictions), 1, nchar(as.character(predictions)) - 3)
dpaccuracy <- sum(substr(as.character(predictions), 1, nchar(as.character(predictions)) - 3) == as.character(test_data[,label_col])) / nrow(test_data)
print(paste("Accuracy:", round(accuracy, 2)))
print(paste("DP Accuracy:", round(dpaccuracy, 2)))
plot(test_data[,c(1,2)], col=test_data[,label_col]+2,ylab="IMC",xlab="Glucosa", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=as.numeric(dpCategPred)+2,ylab="IMC",xlab="Glucosa", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data, col=(test_data[,label_col]==CategPred)+2, cex=1.5, pch=19)
plot(test_data, col=(test_data[,label_col]==dpCategPred)+2, cex=1.5, pch=19)
plot(test_data[,c(1,2)], col=test_data[,label_col]+2,ylab="IMC",xlab="Glucosa", cex=1.5, pch=19)
plot(test_data[,c(1,2)], col=as.numeric(dpCategPred)+2,ylab="IMC",xlab="Glucosa", cex=1.5, pch=19)
epsilon=0.5
epsilon=0.5
n=10000
#Calculo de Max y Min
Upper=rep(0,ncol(iris)-1)
Lower=rep(0,ncol(iris)-1)
for (j in 1:(ncol(iris)-1)) {
Upper[j]=max(iris[j])
Lower[j]=min(iris[j])
}
Smean=rep(0,ncol(iris)-1)
Ssd=rep(0,ncol(iris)-1)
#Sensibilidades
for (j in 1:(ncol(iris)-1)) {
Smean[j]=(Upper[j]-Lower[j])/(n+1)
Ssd[j]=sqrt(n)*(Upper[j]-Lower[j])/(n+1)
}
#En el siguiente ciclo, construrimos stats y guarda la media, sd en cada clase
for (label in unique(train_data[[label_col]])) {
subset <- train_data[train_data[[label_col]] == label,]
stats[[as.character(label)]] <- lapply(subset[, -label_col], function(x) c(mean = mean(x), sd = sd(x)))
}
#Statsdp
statsdp <- stats
for (i in 1:length(stats)) {
for (j in 1:(ncol(iris)-1)) {
statsdp[[i]][[j]][["mean"]]=stats[[i]][[j]][["mean"]]+rlaplace(1,0,Smean[j]/epsilon)
statsdp[[i]][[j]][["sd"]]=stats[[i]][[j]][["sd"]]+rlaplace(1,0,Ssd[j]/epsilon)
}
}
# Calcular probabilidades a priori ( p(c_i)  )
total <- nrow(train_data)
priors <- table(train_data[[label_col]]) / total
priors <- as.list(priors)
# Calcular la probabilidad de la distribución normal
gaussian_prob <- function(x, mean, sd) {
if (sd == 0) {
return(ifelse(x == mean, 1, 0))
}
return((1 / (sqrt(2 * pi) * sd)) * exp(-((x - mean)^2 / (2 * sd^2))))
}
# Predecir las etiquetas del conjunto de prueba
ProbaPost<- list()
predictions=list()
for (i in 1:nrow(test_data)){
x <- test_data[i, -label_col]
# prob a posteiori (solo numerador)
#esto es (P(c_j|X_1,\dots X_n)=P(c_j)\prod P(X_i|c_j))
posteriors <- list()
for (label in names(statsdp)) {
likelihood <- 1
for (feature in names(statsdp[[label]])) {
prob <- gaussian_prob(x[[feature]], statsdp[[label]][[feature]]["mean"], statsdp[[label]][[feature]]["sd"])
likelihood <- likelihood * prob
}
posteriors[[label]] <- likelihood * priors[[label]]
}
ProbaPost[[i]]=posteriors
predictions[i]=names(which.max(unlist(posteriors)))
}
# Verificar las predicciones y la precisión
print(ProbaPost)
print(predictions)
print(test_data[,label_col])
dpCategPred=substr(as.character(predictions), 1, nchar(as.character(predictions)) - 3)
dpaccuracy <- sum(substr(as.character(predictions), 1, nchar(as.character(predictions)) - 3) == as.character(test_data[,label_col])) / nrow(test_data)
print(paste("Accuracy:", round(accuracy, 2)))
print(paste("DP Accuracy:", round(dpaccuracy, 2)))
plot(test_data[,c(1,2)], col=test_data[,label_col]+2,ylab="IMC",xlab="Glucosa", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
plot(test_data[,c(1,2)], col=as.numeric(dpCategPred)+2,ylab="IMC",xlab="Glucosa", cex=1.5, pch=19)
legend("topleft", legend = c("No Positivo", "Positivo"),
col = c(2, 3), pch = 19, cex = 1.2)
